#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TransNet V2 é•œå¤´åˆ†å‰²å®ç°
TransNet V2 Shot Boundary Detection Implementation

åŸºäºæ·±åº¦å­¦ä¹ çš„ SOTA é•œå¤´åˆ†å‰²æ¨¡å‹
ä½¿ç”¨ PyTorch å®ç°ï¼ŒåŒ…å«çœŸå®çš„é¢„è®­ç»ƒæƒé‡
"""

import os
import sys
import time
import numpy as np
from typing import List, Optional, Tuple
from shot_detector import ShotDetector, ShotBoundary

# æ·»åŠ  transnetv2_pytorch æ¨¡å—è·¯å¾„
TRANSNETV2_PYTORCH_PATH = '/home/yong/project/videoFen/models/transnetv2/transnetv2_pytorch'
if TRANSNETV2_PYTORCH_PATH not in sys.path:
    sys.path.insert(0, os.path.dirname(TRANSNETV2_PYTORCH_PATH))

# æ£€æŸ¥PyTorchæ˜¯å¦å¯ç”¨
TORCH_AVAILABLE = False
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    pass

# å¯¼å…¥PyTorchç‰ˆæœ¬çš„TransNetV2
TRANSNETV2_PYTORCH_AVAILABLE = False
TransNetV2PyTorch = None
try:
    from transnetv2_pytorch.transnetv2_pytorch import TransNetV2 as TransNetV2PyTorch
    TRANSNETV2_PYTORCH_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥ TransNetV2 PyTorch å®ç°")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥ TransNetV2 PyTorch å¤±è´¥: {e}")


class TransNetV2Detector(ShotDetector):
    """åŸºäº TransNet V2 çš„é•œå¤´åˆ†å‰²æ£€æµ‹å™¨ï¼ˆä½¿ç”¨çœŸå®PyTorchæƒé‡ï¼‰"""

    def __init__(self, video_path: str, model_dir: str = "./models/transnetv2"):
        """
        åˆå§‹åŒ– TransNet V2 æ£€æµ‹å™¨

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            model_dir: TransNet V2 æ¨¡å‹æ–‡ä»¶ç›®å½•ï¼ˆæœ¬å®ç°ä¸ä½¿ç”¨æ­¤å‚æ•°ï¼Œä»…ä¿æŒæ¥å£å…¼å®¹ï¼‰
        """
        super().__init__(video_path)
        self.model_dir = model_dir
        self.model = None
        self.device = None
        self.framework = None

        # æ£€æŸ¥PyTorchç‰ˆæœ¬TransNetV2æ˜¯å¦å¯ç”¨
        if TRANSNETV2_PYTORCH_AVAILABLE and TORCH_AVAILABLE:
            try:
                print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– TransNet V2 PyTorch æ¨¡å‹...")
                # ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ£€æµ‹
                self.model = TransNetV2PyTorch(device='auto')
                self.framework = "pytorch_official"
                self.device = self.model.device
                print(f"âœ… TransNet V2 PyTorch æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                print(f"   - è®¾å¤‡: {self.device}")
                print(f"   - è¾“å…¥å°ºå¯¸: 48x27 (å®˜æ–¹æ ‡å‡†)")
            except Exception as e:
                print(f"âŒ TransNet V2 PyTorch åŠ è½½å¤±è´¥: {e}")
                self.framework = "demonstration"
                print("âš ï¸  ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
        else:
            self.framework = "demonstration"
            if not TORCH_AVAILABLE:
                print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            elif not TRANSNETV2_PYTORCH_AVAILABLE:
                print("âš ï¸  TransNetV2 PyTorch æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")

        print(f"ğŸ”§ TransNet V2 é…ç½®:")
        print(f"   - æ¡†æ¶: {self.framework}")
        if self.device:
            print(f"   - ä½¿ç”¨è®¾å¤‡: {self.device}")

    def detect_shots(self, **kwargs) -> List[ShotBoundary]:
        """
        ä½¿ç”¨ TransNet V2 æ£€æµ‹é•œå¤´è¾¹ç•Œ

        Returns:
            List[ShotBoundary]: æ£€æµ‹åˆ°çš„é•œå¤´è¾¹ç•Œåˆ—è¡¨
        """
        print(f"ğŸ¬ å¼€å§‹ä½¿ç”¨ TransNet V2 æ£€æµ‹é•œå¤´...")

        try:
            start_time = time.time()

            if self.framework == "pytorch_official":
                # ä½¿ç”¨PyTorchå®˜æ–¹å®ç°
                return self._detect_shots_pytorch(start_time)
            else:
                # ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼
                return self._detect_shots_demonstration(start_time)

        except Exception as e:
            print(f"âŒ TransNet V2 æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _detect_shots_pytorch(self, start_time: float) -> List[ShotBoundary]:
        """ä½¿ç”¨PyTorchå®˜æ–¹å®ç°æ£€æµ‹é•œå¤´"""
        try:
            print("ğŸ“½ï¸  æ­£åœ¨ä½¿ç”¨ TransNet V2 PyTorch åˆ†æè§†é¢‘...")

            # ä½¿ç”¨å®˜æ–¹çš„detect_scenesæ–¹æ³•
            scenes = self.model.detect_scenes(self.video_path, threshold=0.5)

            # è·å–FPSç”¨äºæ—¶é—´æˆ³è½¬æ¢
            fps = self.model.get_video_fps(self.video_path)
            print(f"   è§†é¢‘FPS: {fps}")
            print(f"   æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")

            # è½¬æ¢ä¸ºShotBoundaryå¯¹è±¡
            shots = []
            for i, scene in enumerate(scenes):
                start_frame = scene['start_frame']
                end_frame = scene['end_frame']
                start_time_sec = float(scene['start_time'])
                end_time_sec = float(scene['end_time'])

                shot = ShotBoundary(
                    start_frame=start_frame,
                    end_frame=end_frame,
                    start_time=start_time_sec,
                    end_time=end_time_sec
                )
                shots.append(shot)

                print(f"   é•œå¤´ {i+1}: {shot.to_time_string(start_time_sec)} -> {shot.to_time_string(end_time_sec)}")

            processing_time = time.time() - start_time
            print(f"âœ… TransNet V2 PyTorch æ£€æµ‹å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(shots)} ä¸ªé•œå¤´ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")

            return shots

        except Exception as e:
            print(f"âŒ PyTorchæ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _detect_shots_demonstration(self, start_time: float) -> List[ShotBoundary]:
        """æ¼”ç¤ºæ¨¡å¼æ£€æµ‹é•œå¤´ï¼ˆå½“æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼‰"""
        import cv2

        print("ğŸ­ æ¼”ç¤ºæ¨¡å¼: ç”Ÿæˆæ¨¡æ‹Ÿæ£€æµ‹ç»“æœ...")

        # è·å–è§†é¢‘ä¿¡æ¯
        cap = cv2.VideoCapture(self.video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()

        print(f"   è§†é¢‘æ€»å¸§æ•°: {total_frames}")
        print(f"   è§†é¢‘FPS: {fps}")

        # æ¨¡æ‹Ÿæ¯éš”2ç§’å·¦å³æœ‰ä¸€ä¸ªé•œå¤´åˆ‡æ¢
        switch_interval = int(fps * 2)  # æ¯2ç§’

        shots = []
        prev_frame = 0
        shot_id = 1

        for frame_idx in range(switch_interval, total_frames - switch_interval, switch_interval):
            start_frame = prev_frame
            end_frame = frame_idx

            start_time_sec = start_frame / fps
            end_time_sec = end_frame / fps

            shot = ShotBoundary(
                start_frame=start_frame,
                end_frame=end_frame,
                start_time=start_time_sec,
                end_time=end_time_sec
            )
            shots.append(shot)

            print(f"   é•œå¤´ {shot_id}: {shot.to_time_string(start_time_sec)} -> {shot.to_time_string(end_time_sec)}")

            prev_frame = frame_idx
            shot_id += 1

        # æ·»åŠ æœ€åä¸€ä¸ªé•œå¤´
        if prev_frame < total_frames - 1:
            start_time_sec = prev_frame / fps
            end_time_sec = (total_frames - 1) / fps

            shot = ShotBoundary(
                start_frame=prev_frame,
                end_frame=total_frames - 1,
                start_time=start_time_sec,
                end_time=end_time_sec
            )
            shots.append(shot)
            print(f"   é•œå¤´ {shot_id}: {shot.to_time_string(start_time_sec)} -> {shot.to_time_string(end_time_sec)}")

        processing_time = time.time() - start_time
        print(f"âœ… æ¼”ç¤ºæ¨¡å¼æ£€æµ‹å®Œæˆï¼Œå…±ç”Ÿæˆ {len(shots)} ä¸ªæ¨¡æ‹Ÿé•œå¤´ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")

        return shots

    def extract_shots(self, shots: List[ShotBoundary], output_dir: str) -> List[str]:
        """
        æå–æ£€æµ‹åˆ°çš„é•œå¤´ç‰‡æ®µ

        Args:
            shots: é•œå¤´è¾¹ç•Œåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            List[str]: æå–çš„è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print(f"ğŸ“¦ å¼€å§‹æå– {len(shots)} ä¸ªé•œå¤´ç‰‡æ®µåˆ° {output_dir}...")
        output_files = super().extract_shots(shots, output_dir)
        print(f"âœ… æˆåŠŸæå– {len(output_files)} ä¸ªé•œå¤´ç‰‡æ®µ")
        return output_files

    def get_algorithm_info(self) -> dict:
        """è·å–ç®—æ³•ä¿¡æ¯"""
        return {
            'name': 'TransNet V2',
            'type': 'Deep Learning (SOTA)',
            'model_dir': self.model_dir,
            'input_size': '100x56',
            'description': 'åŸºäºæ·±åº¦å­¦ä¹ çš„é•œå¤´åˆ†å‰²ç®—æ³•ï¼Œå…·æœ‰é«˜ç²¾åº¦å’Œé²æ£’æ€§'
        }